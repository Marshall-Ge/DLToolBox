## Implemented Works

### Models
- [x] [BLIP](models/blip.py)
- [x] [CLIP](models/clip.py)

### Components
- [x] [Cross Attention](models/attention.py)
- [x] [Cross Attention with Multi-Head](models/attention.py)
- [x] [Vision Transformer](models/transformer.py)

## Environment Setup
Using conda
```bash
conda create -n my_env python=3.11
conda activate my_env
```

```bash
pip install -r requirements.txt
```
Using uv
```bash
uv sync
```

## References
1. [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n.html)
